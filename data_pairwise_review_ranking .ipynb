{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# Integrate tqdm with Pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "df = pd.read_csv('data/Featured_Sports_and_Outdoors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign relevance scores based on features\n",
    "def relevance_score(row):\n",
    "    return row['rating'] * 0.4 + row['sentiment_score'] * 0.3 + row['normalized_helpful_votes'] * 0.3\n",
    "\n",
    "df['relevance_score'] = df.progress_apply(relevance_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CSV file\n",
    "csv_filename = 'pair_labels.csv'\n",
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['pair_index1', 'pair_index2', 'label'])\n",
    "\n",
    "# Batch processing parameters\n",
    "batch_size = 1000\n",
    "\n",
    "# Process pairs in batches\n",
    "for start in tqdm(range(0, len(df.index), batch_size), desc=\"Processing Batches\"):\n",
    "    batch_indices = df.index[start:start+batch_size]\n",
    "    \n",
    "    # Generate pairs within the batch\n",
    "    batch_pairs = list(combinations(batch_indices, 2))\n",
    "    \n",
    "    # Open CSV file in append mode\n",
    "    with open(csv_filename, 'a', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Process each pair in the batch\n",
    "        for i, j in batch_pairs:\n",
    "            if df.at[i, 'relevance_score'] > df.at[j, 'relevance_score']:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            \n",
    "            # Write to CSV\n",
    "            csv_writer.writerow([i, j, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Pairs DataFrame:   0%|          | 56/45485 [00:57<12:56:15,  1.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28mlist\u001b[39m(tqdm(executor\u001b[38;5;241m.\u001b[39mmap(process_batch, \u001b[38;5;28mrange\u001b[39m(num_batches)), total\u001b[38;5;241m=\u001b[39mnum_batches, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating Pairs DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 43\u001b[0m \u001b[43mcreate_pairs_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair_labels_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 41\u001b[0m, in \u001b[0;36mcreate_pairs_dataframe\u001b[0;34m(df, pair_labels_df, output_file, batch_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Using ThreadPoolExecutor to parallelize batch processing\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCreating Pairs DataFrame\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment/lib/python3.12/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment/lib/python3.12/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment/lib/python3.12/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "csv_filename = 'pair_labels.csv'\n",
    "pair_labels_df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Prepare data for training\n",
    "def create_pairs_dataframe(df, pair_labels_df, output_file='pairs_data.csv', batch_size=1000):\n",
    "    num_batches = (len(pair_labels_df) // batch_size) + 1\n",
    "    \n",
    "    def process_batch(batch_idx):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, len(pair_labels_df))\n",
    "        batch_pair_labels = pair_labels_df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        batch_pairs_data = []\n",
    "        for idx, row in batch_pair_labels.iterrows():\n",
    "            i = row['pair_index1']\n",
    "            j = row['pair_index2']\n",
    "            label = row['label']\n",
    "            \n",
    "            review_1_data = df.loc[i].to_dict()\n",
    "            review_2_data = df.loc[j].to_dict()\n",
    "            \n",
    "            batch_pairs_data.append({\n",
    "                'review_1': review_1_data,\n",
    "                'review_2': review_2_data,\n",
    "                'label': label\n",
    "            })\n",
    "        \n",
    "        # Convert batch data to DataFrame\n",
    "        batch_df = pd.DataFrame(batch_pairs_data)\n",
    "        \n",
    "        # Append batch data to the CSV file\n",
    "        if batch_idx == 0:\n",
    "            batch_df.to_csv(output_file, index=False, mode='w', header=True)\n",
    "        else:\n",
    "            batch_df.to_csv(output_file, index=False, mode='a', header=False)\n",
    "    \n",
    "    # Using ThreadPoolExecutor to parallelize batch processing\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        list(tqdm(executor.map(process_batch, range(num_batches)), total=num_batches, desc=\"Creating Pairs DataFrame\"))\n",
    "\n",
    "create_pairs_dataframe(df, pair_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df = pd.read_csv('pairs_data.csv')\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_df, test_df = train_test_split(pairs_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data for XGBoost\n",
    "def create_xgb_data(df):\n",
    "    X = []\n",
    "    y = []\n",
    "    for _, row in df.iterrows():\n",
    "        features = []\n",
    "        for feature in ['rating', 'word_count', 'avg_sentence_length', 'sentiment_score', 'review_length', 'normalized_helpful_votes']:\n",
    "            features.append(row['review_1'][feature] - row['review_2'][feature])\n",
    "        X.append(features)\n",
    "        y.append(row['label'])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_xgb_data(train_df)\n",
    "X_test, y_test = create_xgb_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model with GPU\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'objective': 'rank:pairwise',\n",
    "    'eta': 0.1,\n",
    "    'gamma': 1.0,\n",
    "    'min_child_weight': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'eval_metric': 'ndcg'\n",
    "}\n",
    "\n",
    "bst = xgb.train(params, dtrain, num_boost_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "y_pred = bst.predict(dtest)\n",
    "ndcg = ndcg_score([y_test], [y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'NDCG Score: {ndcg}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
